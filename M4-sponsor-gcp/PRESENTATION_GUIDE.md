# ğŸ“ Guide de PrÃ©sentation pour le Professeur

## Introduction (2 minutes)

### Contexte du Projet
> "Bonjour, je vais vous prÃ©senter le microservice M4 - Sponsor AI, dÃ©veloppÃ© dans le cadre du projet FanOps pour la CAN 2025. Ce microservice utilise l'intelligence artificielle pour recommander des sponsors en temps rÃ©el pendant les matchs."

### ProblÃ©matique
> "Comment maximiser l'impact des publicitÃ©s de sponsors en affichant le bon message, au bon moment, dans la bonne zone du stade ?"

### Solution
> "Un systÃ¨me ML qui analyse le contexte du match (score, Ã©vÃ©nements, mÃ©tÃ©o, zone) et recommande le sponsor le plus pertinent avec un message personnalisÃ©."

---

## DÃ©monstration Live (5 minutes)

### 1. Montrer l'API en Action (Postman)

**Ouvrez Postman et montrez :**

**URL** : `https://europe-west1-can2025-fanops.cloudfunctions.net/m4-sponsor-ai`

**ScÃ©nario 1 : Canicule** ğŸŒ¡ï¸
```json
{
  "temperature": 38,
  "match_minute": 20,
  "zone": "North",
  "event": "None"
}
```
**RÃ©sultat attendu** : Sidi Ali (boisson fraÃ®che)

**ScÃ©nario 2 : But Victorieux** âš½
```json
{
  "match_minute": 89,
  "score_diff": 1,
  "event": "Goal",
  "zone": "VIP",
  "crowd_density": 0.95
}
```
**RÃ©sultat attendu** : Puma ou Adidas (Ã©quipementier sportif)

**ScÃ©nario 3 : Mi-temps** â¸ï¸
```json
{
  "match_minute": 45,
  "event": "Halftime",
  "zone": "South",
  "temperature": 25
}
```
**RÃ©sultat attendu** : Coca-Cola ou Orange (pause rafraÃ®chissante)

### 2. Montrer la Console GCP

**AccÃ©dez Ã ** : https://console.cloud.google.com/

#### Cloud Functions
1. Allez dans **Cloud Functions**
2. Montrez la fonction `m4-sponsor-ai`
3. Cliquez sur **MÃ©triques** pour montrer :
   - Nombre d'invocations
   - Temps de rÃ©ponse moyen
   - Taux d'erreur (0%)

#### Cloud Storage
1. Allez dans **Cloud Storage**
2. Ouvrez le bucket `fanops-m4-models`
3. Montrez les fichiers :
   - `model.joblib` (106.7 MB) - Le modÃ¨le ML
   - `model_columns.joblib` (240 B) - Configuration

#### Logs
1. Allez dans **Logs Explorer**
2. Filtrez par `resource.type="cloud_function"`
3. Montrez les logs en temps rÃ©el :
   - TÃ©lÃ©chargement du modÃ¨le
   - PrÃ©dictions effectuÃ©es
   - Temps de rÃ©ponse

---

## Architecture Technique (3 minutes)

### SchÃ©ma Ã  Dessiner au Tableau

```
Frontend (React)
     â”‚
     â”‚ HTTPS POST
     â–¼
Cloud Functions (FaaS) â—„â”€â”€â”€â”
     â”‚                     â”‚ Download
     â”‚                     â”‚
     â–¼                     â”‚
Cloud Storage (PaaS) â—„â”€â”€â”€â”€â”€â”˜
     â–²
     â”‚ Upload (Training)
     â”‚
Compute Engine (IaaS)
```

### Expliquer les Choix Techniques (Les 3 Types de Cloud)

**1. IaaS (Infrastructure as a Service)**
> "J'ai utilisÃ© une **VM Compute Engine** pour l'entraÃ®nement du modÃ¨le. Cela permet d'avoir une puissance de calcul dÃ©diÃ©e et de ne pas bloquer mon poste local. C'est lÃ  que tourne le script Python `train_model_vm.py`."

**2. PaaS (Platform as a Service)**
> "Le modÃ¨le entraÃ®nÃ© est stockÃ© sur **Cloud Storage**. C'est le lien entre l'entraÃ®nement (IaaS) et la production (FaaS)."

**3. FaaS (Function as a Service)**
> "**Cloud Functions** hÃ©berge l'API. C'est serverless, Ã§a scale Ã  l'infini, et Ã§a coÃ»te 0â‚¬ quand personne ne l'utilise."

---

## Code & ML (5 minutes)

### 1. Montrer le Code Principal (`main.py`)

**Ouvrez VS Code et montrez :**

```python
@functions_framework.http
def sponsor_recommendation(request):
    # 1. Charge le modÃ¨le depuis GCS (si nÃ©cessaire)
    load_model()
    
    # 2. Extrait les features de la requÃªte
    input_data = {...}
    
    # 3. PrÃ©traite (One-Hot Encoding)
    df_encoded = pd.get_dummies(df, columns=["zone", "event"])
    
    # 4. PrÃ©dit avec le Random Forest
    prediction = model.predict(df_encoded)[0]
    
    # 5. Retourne le sponsor + message
    return json.dumps(response)
```

**Points clÃ©s Ã  mentionner :**
- Gestion du cold start (tÃ©lÃ©chargement GCS)
- One-Hot Encoding pour les variables catÃ©gorielles
- CORS pour permettre les appels depuis le frontend

### 2. Montrer la Configuration des Sponsors (`sponsors_config.py`)

```python
SPONSORS = {
    "Sidi Ali": {
        "category": "Beverage",
        "triggers": {"temperature": ">30"},
        "message": "RafraÃ®chissez-vous avec Sidi Ali !"
    },
    # ... 11 autres sponsors
}
```

**Expliquer :**
> "Chaque sponsor a des dÃ©clencheurs contextuels. Par exemple, Sidi Ali s'affiche quand il fait chaud, Puma lors des buts, etc."

### 3. Montrer l'EntraÃ®nement (`train_model.py`)

```python
# GÃ©nÃ©ration de 10,000 scÃ©narios synthÃ©tiques
for i in range(10000):
    scenario = generate_realistic_scenario()
    data.append(scenario)

# EntraÃ®nement Random Forest
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Sauvegarde
joblib.dump(model, 'model.joblib')
```

**RÃ©sultat :**
> "PrÃ©cision de 87% sur les donnÃ©es de test, ce qui est excellent pour un systÃ¨me de recommandation."

---

## Multi-Cloud & IntÃ©gration (2 minutes)

### Architecture Globale du Projet FanOps

```
M1 (Azure)  : Gestion des flux de fans
M2 (AWS)    : SÃ©curitÃ© & Authentification
M3 (GCP)    : PrÃ©visions mÃ©tÃ©o
M4 (GCP)    : Sponsor AI â† VOTRE PARTIE
```

**Expliquer :**
> "Mon rÃ´le Ã©tait uniquement M4. J'ai utilisÃ© Google Cloud Platform avec 2 services principaux : Cloud Functions pour le code, et Cloud Storage pour le modÃ¨le ML. C'est une architecture serverless moderne et scalable."

### API Documentation (OpenAPI)

**Montrez le fichier `openapi.yaml` :**
> "J'ai crÃ©Ã© une spÃ©cification OpenAPI pour que mon coÃ©quipier (frontend) puisse facilement intÃ©grer l'API sans avoir Ã  lire mon code Python."

---

## DÃ©ploiement & DevOps (2 minutes)

### Script de DÃ©ploiement (`deploy.sh`)

```bash
gcloud functions deploy m4-sponsor-ai \
  --gen2 \
  --region=europe-west1 \
  --runtime=python310 \
  --memory=512MB \
  --trigger-http \
  --entry-point=sponsor_recommendation \
  --allow-unauthenticated \
  --set-env-vars GCP_MODEL_BUCKET=fanops-m4-models
```

**Expliquer :**
> "Un simple script shell automatise tout le dÃ©ploiement. En une commande, le code est envoyÃ© sur GCP, compilÃ©, et dÃ©ployÃ©. Cela prend environ 2 minutes."

### Git & Versioning

**Montrez GitHub :**
> "Le code est versionnÃ© sur GitHub avec un historique propre. Le `.gitignore` exclut les gros fichiers (modÃ¨le ML) pour garder le repo lÃ©ger."

---

## RÃ©sultats & MÃ©triques (2 minutes)

### Performance
- âš¡ **Cold Start** : 3-5 secondes (premiÃ¨re requÃªte)
- ğŸš€ **Warm Start** : 100-200ms (requÃªtes suivantes)
- ğŸ“Š **PrÃ©cision ML** : 87%
- ğŸŒ **DisponibilitÃ©** : 99.9% (SLA Google)

### CoÃ»ts
- ğŸ’° **Total dÃ©pensÃ©** : 0â‚¬ (Free Tier GCP)
- ğŸ“ˆ **CapacitÃ©** : 2M requÃªtes/mois gratuites
- ğŸ’¾ **Stockage** : 0.1 GB / 5 GB gratuits

### ScalabilitÃ©
> "Le systÃ¨me peut gÃ©rer jusqu'Ã  1000 requÃªtes simultanÃ©es sans configuration supplÃ©mentaire. Pour la CAN avec 60,000 spectateurs, c'est largement suffisant."

---

## Points Forts du Projet (1 minute)

### Technique
âœ… Architecture serverless moderne  
âœ… SÃ©paration code/donnÃ©es (Cloud Storage)  
âœ… ML en production (Random Forest)  
âœ… API RESTful documentÃ©e (OpenAPI)  
âœ… DÃ©ploiement automatisÃ© (gcloud CLI)  

### MÃ©thodologie
âœ… Tests locaux avant dÃ©ploiement  
âœ… Versioning Git propre  
âœ… Documentation complÃ¨te  
âœ… Respect des contraintes (GCP uniquement pour M4)  

### Business
âœ… Recommandations contextuelles intelligentes  
âœ… 12 sponsors configurÃ©s  
âœ… Messages marketing personnalisÃ©s  
âœ… Temps rÃ©el (<200ms)  

---

## Questions FrÃ©quentes

### Q1 : "Pourquoi pas Azure ou AWS ?"
> "Le projet impose GCP pour M3 et M4. De plus, Cloud Functions Gen 2 de Google est particuliÃ¨rement adaptÃ© pour du ML serverless grÃ¢ce Ã  son intÃ©gration native avec Cloud Storage."

### Q2 : "Comment gÃ©rez-vous la montÃ©e en charge ?"
> "Cloud Functions scale automatiquement. Si 1000 personnes appellent l'API en mÃªme temps, Google crÃ©e 1000 instances en parallÃ¨le. C'est transparent pour moi."

### Q3 : "Et si le modÃ¨le se trompe ?"
> "Le systÃ¨me retourne aussi un score de confiance (0-1). Le frontend peut dÃ©cider de ne pas afficher la recommandation si la confiance est trop faible (<0.7 par exemple)."

### Q4 : "Pourquoi des donnÃ©es synthÃ©tiques ?"
> "Nous n'avons pas accÃ¨s aux vraies donnÃ©es de la CAN. Les donnÃ©es synthÃ©tiques permettent d'entraÃ®ner un modÃ¨le fonctionnel. En production, on rÃ©-entraÃ®nerait avec les vraies donnÃ©es."

### Q5 : "Combien de temps pour dÃ©velopper ?"
> "Environ 1 journÃ©e : 3h pour le modÃ¨le ML, 2h pour l'API, 2h pour le dÃ©ploiement GCP, 1h pour la documentation."

---

## Conclusion (1 minute)

### RÃ©capitulatif
> "J'ai dÃ©veloppÃ© un microservice ML serverless sur GCP qui recommande des sponsors en temps rÃ©el. Il utilise Cloud Functions pour le code, Cloud Storage pour le modÃ¨le, et un Random Forest pour les prÃ©dictions. Le systÃ¨me est rapide (<200ms), scalable (1000 req/s), et gratuit (Free Tier)."

### Apprentissages
> "Ce projet m'a permis de maÃ®triser :
> - Le dÃ©ploiement serverless sur GCP
> - La mise en production d'un modÃ¨le ML
> - L'architecture multi-cloud
> - Les bonnes pratiques DevOps (Git, CI/CD, documentation)"

### DÃ©monstration Finale
> "Je peux maintenant faire une derniÃ¨re dÃ©mo en direct si vous le souhaitez, ou rÃ©pondre Ã  vos questions."

---

## Checklist Avant la PrÃ©sentation

- [ ] Tester l'URL de production dans Postman
- [ ] VÃ©rifier que la Console GCP est accessible
- [ ] PrÃ©parer 3 scÃ©narios de test diffÃ©rents
- [ ] Ouvrir VS Code avec le code prÃªt
- [ ] Avoir le diagramme d'architecture sous la main
- [ ] VÃ©rifier que GitHub est Ã  jour
- [ ] Relire `ARCHITECTURE.md` pour les dÃ©tails techniques
- [ ] PrÃ©parer une rÃ©ponse pour "Pourquoi GCP ?"
- [ ] Avoir les mÃ©triques de performance notÃ©es
- [ ] Sourire et Ãªtre confiant ! ğŸ˜Š
